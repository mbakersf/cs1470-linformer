{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbakersf/cs1470-linformer/blob/main/amazon_title_train_default.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "BF3dfspWNAmZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fmGwwfja49F"
      },
      "outputs": [],
      "source": [
        "!pip install fairseq\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrWqYmVjdXs6",
        "outputId": "dc19725b-c2d9-4d8d-982c-561e518f3e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35184, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 35184 (delta 61), reused 72 (delta 47), pack-reused 35079\u001b[K\n",
            "Receiving objects: 100% (35184/35184), 25.22 MiB | 19.73 MiB/s, done.\n",
            "Resolving deltas: 100% (25548/25548), done.\n",
            "/content/fairseq\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the BPE encoder and vocabulary\n",
        "!wget -N 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json'\n",
        "!wget -N 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'"
      ],
      "metadata": {
        "id": "h6GwLELTM89y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO3DcG79U3I2"
      },
      "source": [
        "## IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU4Ks2c5Ycp9"
      },
      "outputs": [],
      "source": [
        "# Downloading the IMDB dataset\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar zxvf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsjhErEObZpC"
      },
      "outputs": [],
      "source": [
        "# Format data\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "\n",
        "def prepare_data(datadir):\n",
        "    random.seed(0)\n",
        "    for split in ['train', 'test']:\n",
        "        samples = []\n",
        "        for class_label in ['pos', 'neg']:\n",
        "            fnames = glob(os.path.join(datadir, split, class_label) + '/*.txt')\n",
        "            for fname in fnames:\n",
        "                with open(fname, 'r') as fin:\n",
        "                    line = fin.readline().strip()\n",
        "                    samples.append((line, 1 if class_label == 'pos' else 0))\n",
        "        random.shuffle(samples)\n",
        "        out_fname = 'train' if split == 'train' else 'dev'\n",
        "        with open(os.path.join(datadir, out_fname + '.input0'), 'w') as f1, \\\n",
        "             open(os.path.join(datadir, out_fname + '.label'), 'w') as f2:\n",
        "            for sample in samples:\n",
        "                f1.write(sample[0] + '\\n')\n",
        "                f2.write(str(sample[1]) + '\\n')\n",
        "\n",
        "prepare_data('aclImdb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u6Ssdu_cVm3"
      },
      "outputs": [],
      "source": [
        "# BPE encoding of the data\n",
        "!python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
        "    --encoder-json encoder.json \\\n",
        "    --vocab-bpe vocab.bpe \\\n",
        "    --inputs \"../aclImdb/train.input0\" \"../aclImdb/dev.input0\" \\\n",
        "    --outputs \"../aclImdb/train.input0.bpe\" \"../aclImdb/dev.input0.bpe\" \\\n",
        "    --workers 60 \\\n",
        "    --keep-empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs_76f5rccz4"
      },
      "outputs": [],
      "source": [
        "# Download the dictionary for fairseq\n",
        "!wget -N 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt'\n",
        "\n",
        "# Preprocess the data for fairseq\n",
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"../aclImdb/train.input0.bpe\" \\\n",
        "    --validpref \"../aclImdb/dev.input0.bpe\" \\\n",
        "    --destdir \"../IMDB-bin/input0\" \\\n",
        "    --srcdict dict.txt \\\n",
        "    --workers 60\n",
        "\n",
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"../aclImdb/train.label\" \\\n",
        "    --validpref \"../aclImdb/dev.label\" \\\n",
        "    --destdir \"../IMDB-bin/label\" \\\n",
        "    --workers 60\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHJyrOEaciFB"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "\n",
        "!fairseq-train \"/content/IMDB-bin/\" \\\n",
        "    --user-dir /content/fairseq/examples/linformer/linformer_src \\\n",
        "    --max-positions 512 \\\n",
        "    --batch-size 16 \\\n",
        "    --max-tokens 4400 \\\n",
        "    --task sentence_prediction \\\n",
        "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "    --required-batch-size-multiple 1 \\\n",
        "    --init-token 0 --separator-token 2 \\\n",
        "    --arch linformer_roberta_base \\\n",
        "    --criterion sentence_prediction \\\n",
        "    --classification-head-name 'imdb_head' \\\n",
        "    --num-classes 2 \\\n",
        "    --dropout 0.1 --attention-dropout 0.1 \\\n",
        "    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr-scheduler polynomial_decay --lr 1e-05 --total-num-update 7812 --warmup-updates 469 \\\n",
        "    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
        "    --max-epoch 1 \\\n",
        "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
        "    --shorten-method \"truncate\" \\\n",
        "    --find-unused-parameters \\\n",
        "    --update-freq 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGK9X782bZyV"
      },
      "source": [
        "## Twitter Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRfJrdzKIckj"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset from Kaggle\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'sentiment140:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2477%2F4140%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240503%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240503T155156Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D287bc94575b702a6892fe02b7bd28c326a770f21bf9b3800966a2d7b086fdff55f5a34ec7f9085607ee9b47f7377889ebb681c276883014dae0098dc44c979cef2a1da717476acc16ba0c738273585d2cca0721c12a45af861cd7deb08724952a3f6ca8ba45ed6b55aa0c22fd0054cde92f216e6cf6388c84700e4d22af8a4955a7cd524e6a1e9956d8c8b981be1773b2f187f7b969c87de1c0fca659e5e68570477c3eb52066d453f36e6229e50514d6d21c1d460acdf4d22a2763469dfa07801404cfad4383ff87b396cedf172160af365557cebe9b40ec9a1838af60e49b5a8eb8ada61146332bb53e6e0db21087781df57d2613fa4a816bf965739a546dd'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvbiFpf3LC2J"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv',encoding = 'latin',header=None)\n",
        "data = data[[5, 0]]\n",
        "data = data.head(10000)\n",
        "data.columns=['tweet', 'sentiment']\n",
        "print(data.head())\n",
        "data['sentiment'] = data['sentiment'].replace(4,1)\n",
        "data = data.sample(frac = 1)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXHc9qI-V_mE",
        "outputId": "76709c38-d432-481a-9bf6-00d1f0638775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training texts: 144000\n",
            "Training labels: 144000\n",
            "Validation texts: 16000\n",
            "Validation labels: 16000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "train_df, dev_df = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "train_df['tweet'].to_csv('train.input0', index=False, header=False)\n",
        "train_df['sentiment'].to_csv('train.label', index=False, header=False)\n",
        "dev_df['tweet'].to_csv('dev.input0', index=False, header=False)\n",
        "dev_df['sentiment'].to_csv('dev.label', index=False, header=False)\n",
        "\n",
        "print(\"Training texts:\", len(open('train.input0').readlines()))\n",
        "print(\"Training labels:\", len(open('train.label').readlines()))\n",
        "print(\"Validation texts:\", len(open('dev.input0').readlines()))\n",
        "print(\"Validation labels:\", len(open('dev.label').readlines()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXFq4wJPcRZW"
      },
      "outputs": [],
      "source": [
        "# # BPE encoding of the data\n",
        "!python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
        "    --encoder-json encoder.json \\\n",
        "    --vocab-bpe vocab.bpe \\\n",
        "    --inputs \"train.input0\" \"dev.input0\" \\\n",
        "    --outputs \"train.input0.bpe\" \"dev.input0.bpe\" \\\n",
        "    --workers 60 \\\n",
        "    --keep-empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odFtgksNy52k"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data for fairseq\n",
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"train.input0.bpe\" \\\n",
        "    --validpref \"dev.input0.bpe\" \\\n",
        "    --destdir \"Tweet-bin/input0\" \\\n",
        "    --srcdict dict.txt \\\n",
        "    --workers 60\n",
        "\n",
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"train.label\" \\\n",
        "    --validpref \"dev.label\" \\\n",
        "    --destdir \"Tweet-bin/label\" \\\n",
        "    --workers 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9VLDqzPNtMJ"
      },
      "outputs": [],
      "source": [
        "# Re-doing this process with a subset of the dataset\n",
        "!rm -rf Tweet-bin/input0 Tweet-bin/label\n",
        "!head -n 16000 train.label > train.small.label\n",
        "\n",
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"train.input0.bpe\" \\\n",
        "    --validpref \"dev.input0.bpe\" \\\n",
        "    --destdir \"Tweet-bin/input0\" \\\n",
        "    --srcdict dict.txt \\\n",
        "    --workers 60\n",
        "\n",
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"train.small.label\" \\\n",
        "    --validpref \"dev.label\" \\\n",
        "    --destdir \"Tweet-bin/label\" \\\n",
        "    --workers 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiXZaLe7WOlQ"
      },
      "outputs": [],
      "source": [
        "# Training with Linformer using the Twitter data\n",
        "!fairseq-train \"Tweet-bin/\" \\\n",
        "    --user-dir /content/fairseq/examples/linformer/linformer_src \\\n",
        "    --max-positions 512 \\\n",
        "    --batch-size 16 \\\n",
        "    --max-tokens 4400 \\\n",
        "    --task sentence_prediction \\\n",
        "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "    --required-batch-size-multiple 1 \\\n",
        "    --init-token 0 --separator-token 2 \\\n",
        "    --arch linformer_roberta_base \\\n",
        "    --criterion sentence_prediction \\\n",
        "    --classification-head-name 'sentiment_head' \\\n",
        "    --num-classes 2 \\\n",
        "    --dropout 0.1 --attention-dropout 0.1 \\\n",
        "    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr-scheduler polynomial_decay --lr 1e-05 --total-num-update 7812 --warmup-updates 469 \\\n",
        "    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
        "    --max-epoch 10 \\\n",
        "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
        "    --shorten-method \"truncate\" \\\n",
        "    --find-unused-parameters \\\n",
        "    --update-freq 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ8uc1KDQ2qu"
      },
      "source": [
        "## Amazon Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaaudkoiZzBh"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"amazon_polarity\")\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PrnFwNUaALE"
      },
      "outputs": [],
      "source": [
        "print(dataset)\n",
        "print(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjWEf5XIaS7N"
      },
      "outputs": [],
      "source": [
        "for example in train_data.shuffle(seed=42).select(range(5)):\n",
        "    print(f\"Label: {example['label']}, Review Title: {example['title']}, Review Content: {example['content']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EulIXNzcR-7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df_train = pd.DataFrame(train_data)\n",
        "df_test = pd.DataFrame(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q_35ZxpImEW"
      },
      "outputs": [],
      "source": [
        "print(df_test.head())\n",
        "\n",
        "df_train['label'] = df_train['label'].replace({4: 1})\n",
        "df_test['label'] = df_test['label'].replace({4: 1})\n",
        "\n",
        "print(df_train.head())\n",
        "\n",
        "df_train = df_train.sample(frac=1, random_state=42)\n",
        "df_test = df_test.sample(frac=1, random_state=42)\n",
        "\n",
        "df_train = df_train.head(25000)\n",
        "df_test = df_test.head(25000)\n",
        "\n",
        "print(df_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grwpC6YLdTTr"
      },
      "outputs": [],
      "source": [
        "df_train['title'].to_csv('train.input0', index=False, header=False)\n",
        "df_train['label'].to_csv('train.label', index=False, header=False)\n",
        "df_test['title'].to_csv('dev.input0', index=False, header=False)\n",
        "df_test['label'].to_csv('dev.label', index=False, header=False)\n",
        "\n",
        "print(\"Training texts:\", len(open('train.input0').readlines()))\n",
        "print(\"Training labels:\", len(open('train.label').readlines()))\n",
        "print(\"Validation texts:\", len(open('dev.input0').readlines()))\n",
        "print(\"Validation labels:\", len(open('dev.label').readlines()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBCuZ7-Ddz6Z"
      },
      "outputs": [],
      "source": [
        "# # BPE encoding of the data\n",
        "!rm -rf Amazon-bin/input0 Amazon-bin/label\n",
        "\n",
        "!python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
        "    --encoder-json encoder.json \\\n",
        "    --vocab-bpe vocab.bpe \\\n",
        "    --inputs \"train.input0\" \\\n",
        "    --outputs \"train.input0.bpe\" \\\n",
        "    --workers 60 \\\n",
        "    --keep-empty\n",
        "\n",
        "!python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
        "    --encoder-json encoder.json \\\n",
        "    --vocab-bpe vocab.bpe \\\n",
        "    --inputs \"dev.input0\" \\\n",
        "    --outputs \"dev.input0.bpe\" \\\n",
        "    --workers 60 \\\n",
        "    --keep-empty\n",
        "\n",
        "# Preprocess the data for fairseq\n",
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"train.input0.bpe\" \\\n",
        "    --validpref \"dev.input0.bpe\" \\\n",
        "    --destdir \"Amazon-bin/input0\" \\\n",
        "    --srcdict dict.txt \\\n",
        "    --workers 60\n",
        "\n",
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"train.label\" \\\n",
        "    --validpref \"dev.label\" \\\n",
        "    --destdir \"Amazon-bin/label\" \\\n",
        "    --workers 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDzNbnnXangu"
      },
      "outputs": [],
      "source": [
        "!wget -N 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-xIOEfghYg3"
      },
      "outputs": [],
      "source": [
        "!fairseq-train \"Amazon-bin/\" \\\n",
        "    --user-dir /content/fairseq/examples/linformer/linformer_src \\\n",
        "    --max-positions 512 \\\n",
        "    --batch-size 16 \\\n",
        "    --max-tokens 4400 \\\n",
        "    --task sentence_prediction \\\n",
        "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "    --required-batch-size-multiple 1 \\\n",
        "    --init-token 0 --separator-token 2 \\\n",
        "    --arch linformer_roberta_base \\\n",
        "    --criterion sentence_prediction \\\n",
        "    --classification-head-name 'Amazon_head' \\\n",
        "    --num-classes 2 \\\n",
        "    --dropout 0.1 --attention-dropout 0.1 \\\n",
        "    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr-scheduler polynomial_decay --lr 1e-05 --total-num-update 7812 --warmup-updates 469 \\\n",
        "    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
        "    --max-epoch 10 \\\n",
        "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
        "    --shorten-method \"truncate\" \\\n",
        "    --find-unused-parameters \\\n",
        "    --update-freq 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNWkgn2Va-Zu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Load the model from the .pt file\n",
        "loaded_model = torch.load('checkpoints/checkpoint_best.pt')\n",
        "\n",
        "with open('checkpoint_best.pkl', 'wb') as f:\n",
        "    pickle.dump(loaded_model, f)\n",
        "\n",
        "with open('checkpoint_best.pkl', 'rb') as f:\n",
        "    loaded_model_from_pickle = pickle.load(f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
